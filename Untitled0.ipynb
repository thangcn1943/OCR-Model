{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-BMGR25m_JX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIlQbou_nRad"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Scene text recognition/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYB03zRZu_Pf"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mBtQkDSvPWr"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import yaml\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLuI_Xyane3Q"
      },
      "outputs": [],
      "source": [
        "word_xml_path = \"data/icdar2003/SceneTrialTrain/words.xml\"\n",
        "tree = ET.parse(word_xml_path)\n",
        "root = tree.getroot()\n",
        "print(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO7QAMzPn7Ml"
      },
      "outputs": [],
      "source": [
        "for image in root:\n",
        "    imag_name = image[0].text\n",
        "    for bbs in image.findall('taggedRectangles'):\n",
        "        for bb in bbs:\n",
        "            bbox = [\n",
        "                float(bb.attrib['x']),\n",
        "                float(bb.attrib['y']),\n",
        "                float(bb.attrib['width']),\n",
        "                float(bb.attrib['height'])\n",
        "            ]\n",
        "            print(bb[0].text, bbox)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FwBTRjWnZH9"
      },
      "outputs": [],
      "source": [
        "def extract_from_xml(path):\n",
        "    image_paths = []\n",
        "    image_sizes = []\n",
        "    image_labels = []\n",
        "    bounding_boxes = []\n",
        "    tree = ET.parse(path)\n",
        "    root = tree.getroot()\n",
        "    for image in root:\n",
        "        bbs_of_image = []\n",
        "        labels_of_image = []\n",
        "\n",
        "        for bbs in image.findall('taggedRectangles'):\n",
        "            for bb in bbs:\n",
        "                if not bb[0].text.isalnum():\n",
        "                    continue\n",
        "                if 'é' in bb[0].text.lower() or 'ñ' in bb[0].text.lower():\n",
        "                    continue\n",
        "\n",
        "                bbs_of_image.append(\n",
        "                    [\n",
        "                        float(bb.attrib['x']),\n",
        "                        float(bb.attrib['y']),\n",
        "                        float(bb.attrib['width']),\n",
        "                        float(bb.attrib['height'])\n",
        "                    ]\n",
        "                )\n",
        "                labels_of_image.append(bb[0].text)\n",
        "        image_paths.append(image[0].text)\n",
        "        image_sizes.append((int(image[1].attrib['x']), int(image[1].attrib['y'])))\n",
        "        image_labels.append(labels_of_image)\n",
        "        bounding_boxes.append(bbs_of_image)\n",
        "    return image_paths, image_sizes, image_labels, bounding_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7kbNXD8roim"
      },
      "outputs": [],
      "source": [
        "words_xml_path = os.path.join('data','icdar2003','SceneTrialTrain','words.xml')\n",
        "image_paths, image_sizes, image_labels, bounding_boxes = extract_from_xml(words_xml_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p-JCUu0PubgW"
      },
      "outputs": [],
      "source": [
        "image_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJ-EtIbqS5R3"
      },
      "outputs": [],
      "source": [
        "print(bounding_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofOTXACcuhJO"
      },
      "source": [
        "# **Convert to YOLOv8 format**\n",
        "vì yolo format có định dạng là center và width height và có giá trị trong khoảng từ (0,1) do đó cần normalize lại các giá trị"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZCQ0tTSumBt"
      },
      "outputs": [],
      "source": [
        "def convert_to_yolo_format(image_paths, image_sizes, bounding_boxes):\n",
        "    yolo_data = []\n",
        "\n",
        "    for image_path, image_size, bboxes in zip(image_paths, image_sizes, bounding_boxes):\n",
        "        image_width, image_height = image_size\n",
        "        yolo_labels = []\n",
        "\n",
        "        for bbox in bboxes:\n",
        "            x, y, w, h = bbox\n",
        "\n",
        "            center_x = (x + w / 2) / image_width\n",
        "            center_y = (y + h / 2) / image_height\n",
        "            norm_width = w / image_width\n",
        "            norm_height = h / image_height\n",
        "\n",
        "            class_id = 0\n",
        "\n",
        "            yolo_label = f\"{class_id} {center_x} {center_y} {norm_width} {norm_height}\"\n",
        "            yolo_labels.append(yolo_label)\n",
        "\n",
        "        yolo_data.append((image_path, yolo_labels))\n",
        "\n",
        "    return yolo_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUXFPTZS2yTr"
      },
      "outputs": [],
      "source": [
        "class_labels = ['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afIK5jCv2dzp"
      },
      "outputs": [],
      "source": [
        "yolo_data = convert_to_yolo_format(image_paths, image_sizes, bounding_boxes)\n",
        "yolo_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UchWa4sH4D6K"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMdWS7ZZ3rGD"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "val_size = 0.3\n",
        "test_size = 2/3\n",
        "is_shuffle = True\n",
        "train_data, test_data = train_test_split(\n",
        "    yolo_data,\n",
        "    test_size=val_size,\n",
        "    random_state=seed,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "test_data, val_data = train_test_split(\n",
        "    test_data,\n",
        "    test_size=test_size,\n",
        "    random_state=seed,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjpr6wsF3rcK"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "En7MxWDJ9uGC"
      },
      "outputs": [],
      "source": [
        "def save_data(data, src_img_dir, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    os.makedirs(os.path.join(save_dir,'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_dir,'labels'), exist_ok=True)\n",
        "\n",
        "    for image_path, yolo_labels in data:\n",
        "        shutil.copy(\n",
        "            os.path.join(src_img_dir, image_path),\n",
        "            os.path.join(save_dir, 'images')\n",
        "        )\n",
        "\n",
        "        image_name = os.path.basename(image_path)\n",
        "        image_name = os.path.splitext(image_name)[0]\n",
        "        with open(os.path.join(save_dir,'labels', f\"{image_name}.txt\"), 'w') as f:\n",
        "            for label in yolo_labels:\n",
        "                f.write(f\"{label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYgi6Rzo-9TM"
      },
      "outputs": [],
      "source": [
        "save_yolo_data_dir = 'datasets/yolo_data'\n",
        "\n",
        "os.makedirs(save_yolo_data_dir, exist_ok=True)\n",
        "save_train_dir = os.path.join(save_yolo_data_dir, 'train')\n",
        "save_val_dir = os.path.join(save_yolo_data_dir, 'val')\n",
        "save_test_dir = os.path.join(save_yolo_data_dir, 'test')\n",
        "dataset_dir = 'data/icdar2003/SceneTrialTrain'\n",
        "save_data(train_data, dataset_dir, save_train_dir)\n",
        "save_data(val_data, dataset_dir, save_val_dir)\n",
        "save_data(test_data, dataset_dir, save_test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI29p2UlAhey"
      },
      "outputs": [],
      "source": [
        "data_yaml = {\n",
        "    'path': 'yolo_data',\n",
        "    'train':'train/images',\n",
        "    'val':'val/images',\n",
        "    'test':'test/images',\n",
        "    'nc': 1,\n",
        "    'names':class_labels,\n",
        "}\n",
        "\n",
        "yolo_yaml_path = os.path.join(save_yolo_data_dir ,'data.yml')\n",
        "with open(yolo_yaml_path, 'w') as f:\n",
        "    yaml.dump(data_yaml, f,default_flow_style=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBwFR23ZIub8"
      },
      "outputs": [],
      "source": [
        "print(yolo_yaml_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4qoS2P3BKEi"
      },
      "source": [
        "#**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bmdM5GDfBM3y"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8s.yaml').load('yolov8s.pt')\n",
        "\n",
        "epochs = 200\n",
        "imgsz = 1024\n",
        "results = model.train(\n",
        "    data = yolo_yaml_path,\n",
        "    epochs = epochs,\n",
        "    imgsz = imgsz,\n",
        "    project = 'models',\n",
        "    name = 'yolov8/detect/train',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C3bcPYhCCja"
      },
      "source": [
        "#**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HupMfQInCEg-"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model_path = 'models/yolov8/detect/train/weights/best.pt'\n",
        "yolo_model = YOLO(model_path)\n",
        "\n",
        "metrics = yolo_model.val(\n",
        "    project = 'models',\n",
        "    name = 'yolov8/detect/val',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktg6yminUVcQ"
      },
      "source": [
        "Load thử một ảnh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc16kGNGV5VG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB81dIuHUU0H"
      },
      "outputs": [],
      "source": [
        "def visualize_bbox(img_path, predictions,conf_thres = 0, font = cv2.FONT_HERSHEY_SIMPLEX):\n",
        "    img = cv2.imread(img_path)\n",
        "    h, w = img.shape[:2]\n",
        "\n",
        "    for prediction in predictions:\n",
        "        conf_score = prediction['confidence']\n",
        "\n",
        "        if conf_score < conf_thres:\n",
        "            continue\n",
        "\n",
        "        bbox = prediction['box']\n",
        "        xmin = int(bbox['x1'])\n",
        "        ymin = int(bbox['y1'])\n",
        "        xmax = int(bbox['x2'])\n",
        "        ymax = int(bbox['y2'])\n",
        "\n",
        "        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
        "\n",
        "        text = f\"{conf_score:.2f}\"\n",
        "        (text_width, text_height),_ = cv2.getTextSize(text, font, 1, 2)\n",
        "\n",
        "        cv2.rectangle(img, (xmin,ymin-text_height - 5), (xmax + text_width, ymin),(0, 255, 0), 2)\n",
        "        cv2.putText(img, text, (xmin, ymin - 5), font, 1, (0, 0, 0), 2)\n",
        "\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRCb1IDtVb8Z"
      },
      "outputs": [],
      "source": [
        "model_path = 'models/yolov8/detect/train/weights/best.pt'\n",
        "img_path = 'datasets/yolo_data/train/images/IMG_1263.JPG'\n",
        "\n",
        "conf_thres = 0.75\n",
        "\n",
        "results = model(img_path, verbose= False)\n",
        "predictions = json.loads(results[0].tojson())\n",
        "print(predictions)\n",
        "\n",
        "visualize_img = visualize_bbox(img_path, predictions, conf_thres)\n",
        "plt.imshow(visualize_img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN52YSUZEdBC"
      },
      "source": [
        "# **Text Recognition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbD6LMkcEl3A"
      },
      "source": [
        "Sử dụng mô hình cơ bản trong OCR là CRNN(Convolution Recurrent Neural Network)\n",
        "Vì đây cũng là dạng bài sequence nhưng không nằm ở dạng text mà là ảnh --> vẫn dùng LSTM để thực hiện\n",
        "Ban đầu sẽ đưa qua CNN để bóc tách đặc trưng ảnh.   \n",
        "--> Khi đi qua CNN sẽ có các feature map\n",
        "vd khi flatten sẽ được output cuối cùng khoảng (512,1,1) x N  \n",
        "Idea lúc này: coi N feature vector là cái token thứ Xi, mỗi token sẽ fit qua RNN coi như là text bình thường.  \n",
        "Paper đã chứng minh được mỗi feature vector sẽ đại diện cho 1 phần hình trong ảnh.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDJ5JEQrH6nm"
      },
      "source": [
        "Với mỗi vị trí X đi vào có thể thiết kế mạng LSTM Many-to-Many. Với 1 mỗi X sẽ dự đoán được 1Y\n",
        "Ảnh --> CNN --> RNN --> CTC --> Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4hhCkUjIvH1"
      },
      "source": [
        "Nếu sử dụng CrossEntropy sẽ bị lặp từ và sai khá nhiều.  \n",
        "colab funtionc: giữ lại 1 nếu các kí tự giống nhau.\n",
        "vd: bbobook --> bok vì ghép b và o vs nhau, colab function sẽ không nhận diện được có bao nhiêu chữ o mà chỉ giữ lại 1 chữ --> sai   \n",
        "khi có CTC loss sẽ học được các blank token để ngăn lại colab function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh5g5EHvNvct"
      },
      "source": [
        "1. Cropped images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8sIGjjosx-C"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtK9cib0OcvE"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transform\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNbFwlZzKstm"
      },
      "outputs": [],
      "source": [
        "def split_bounding_boxes(img_paths, img_labels, bboxes, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    count = 0\n",
        "    labels = []\n",
        "    for img_path, img_label, bbs in zip(img_paths, img_labels, bboxes):\n",
        "        img = Image.open(os.path.join(\"data/icdar2003/SceneTrialTrain\",img_path))\n",
        "        for label, bb in zip(img_label, bbs):\n",
        "            cropped_img = img.crop((bb[0], bb[1], bb[0]+bb[2], bb[1]+bb[3]))\n",
        "            if np.mean(cropped_img) < 35 or np.mean(cropped_img) > 220:\n",
        "                continue\n",
        "            if cropped_img.size[0] < 10 or cropped_img.size[1] < 10:\n",
        "                continue\n",
        "            if len(img_label) < 3:\n",
        "                continue\n",
        "\n",
        "            filename = f\"{count:06d}.jpg\"\n",
        "            cropped_img.save(os.path.join(save_dir, filename))\n",
        "            new_img_path = os.path.join(save_dir, filename)\n",
        "            label = new_img_path + '\\t' + label\n",
        "            labels.append(label)\n",
        "            count += 1\n",
        "\n",
        "    print(f\"Created {count} images\")\n",
        "\n",
        "    with open(os.path.join(save_dir, 'labels.txt'), 'w') as f:\n",
        "        for label in labels:\n",
        "            f.write(f\"{label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCuqfrAm7jGE"
      },
      "outputs": [],
      "source": [
        "save_dir = \"datasets/ocr_data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiYXz7n_7gtF"
      },
      "outputs": [],
      "source": [
        "split_bounding_boxes(image_paths, image_labels, bounding_boxes, save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ErcGuQRQe2"
      },
      "outputs": [],
      "source": [
        "root_dir = save_dir\n",
        "img_paths = []\n",
        "labels = []\n",
        "with open(os.path.join(root_dir, 'labels.txt'), 'r') as f:\n",
        "    for label in f:\n",
        "        labels.append(label.strip().split('\\t')[1])\n",
        "        img_paths.append(label.strip().split('\\t')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2zY5StirXbi"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(root_dir, 'labels.txt'), 'r') as f:\n",
        "    for label in f:\n",
        "        print(label)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsX4qtrjVt_B"
      },
      "outputs": [],
      "source": [
        "print(f\"Total images: {len(img_paths)}\")\n",
        "print(f\"Total labels: {len(labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgEbK8iqXKZp"
      },
      "source": [
        "Tạo bộ vocab để recognite các ký tự.  \n",
        "Vì CRNN dự đoán theo từng ký tự --> ký tự aphalbet và các số. Bỏ qua các dấu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK60zEjoXZVk"
      },
      "outputs": [],
      "source": [
        "letters = [char.split('.')[0].lower() for char in labels]\n",
        "\n",
        "letters = \"\".join(letters)\n",
        "letters = sorted(list(set(letters)))\n",
        "print(letters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MW_iuCRXvta"
      },
      "outputs": [],
      "source": [
        "chars = \"\".join(letters)\n",
        "blank_char = '-'\n",
        "chars += blank_char\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocab: {chars}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7k8nDXX5Yn9j"
      },
      "outputs": [],
      "source": [
        "char_to_idx = {char: idx + 1 for idx,char in enumerate(sorted(chars))}\n",
        "print(char_to_idx)\n",
        "max_label_len = max([len(label) for label in labels])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoMWaQvCZ2KO"
      },
      "outputs": [],
      "source": [
        "print(max_label_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0puMlaDkZCBZ"
      },
      "outputs": [],
      "source": [
        "def encode_label(label, char_to_idx, max_label_len):\n",
        "    encoded_label = torch.tensor(\n",
        "        [char_to_idx[char] for char in label.lower()],\n",
        "        dtype = torch.long\n",
        "    )\n",
        "    label_len = len(encoded_label)\n",
        "    lengths = torch.tensor(label_len, dtype = torch.long)\n",
        "    padded_label = F.pad(\n",
        "        encoded_label,\n",
        "        (0, max_label_len - label_len),\n",
        "        value = 0\n",
        "    )\n",
        "    return padded_label, lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfbkMju9bZrY"
      },
      "outputs": [],
      "source": [
        "test = \"Helao\"\n",
        "print(encode_label(test, char_to_idx, max_label_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxBIIwP5a0DO"
      },
      "outputs": [],
      "source": [
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "def decode(encoded_seq, idx_to_char, blank_char ='-'):\n",
        "    decoded_seq = []\n",
        "    for seq in encoded_seq:\n",
        "        decoded_label = []\n",
        "        for (idx, token) in enumerate(seq):\n",
        "            if token != 0:\n",
        "                char = idx_to_char[token.item()]\n",
        "                if char != blank_char:\n",
        "                    decoded_label.append(char)\n",
        "        decoded_seq.append(''.join(decoded_label))\n",
        "    return decoded_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbyVzHAybZEv"
      },
      "outputs": [],
      "source": [
        "encoded_seq = torch.tensor([[19, 16, 23, 23, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
        "print(decode(encoded_seq, idx_to_char))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKocNu1dio1F"
      },
      "source": [
        "data preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNKDZGrqfw7U"
      },
      "outputs": [],
      "source": [
        "data_transform = {\n",
        "    'train' : transform.Compose([\n",
        "        transform.Resize((100,420)),\n",
        "        transform.ColorJitter(\n",
        "            brightness = 0.5,\n",
        "            contrast = 0.5,\n",
        "            saturation = 0.5\n",
        "        ),\n",
        "        transform.Grayscale(num_output_channels = 1),\n",
        "        transform.GaussianBlur(3),\n",
        "        transform.RandomAffine(degrees = 1, shear = 1),\n",
        "        transform.RandomPerspective(\n",
        "            distortion_scale = 0.5,\n",
        "            p = 0.5,\n",
        "            interpolation = 3\n",
        "        ),\n",
        "        transform.RandomRotation(degrees = 2),\n",
        "        transform.ToTensor(),\n",
        "        transform.Normalize((0.5,), (0.5,))\n",
        "    ]),\n",
        "    'val' : transform.Compose([\n",
        "        transform.Resize((100,420)),\n",
        "        transform.Grayscale(num_output_channels=1),\n",
        "        transform.ToTensor(),\n",
        "        transform.Normalize((0.5,), (0.5,))\n",
        "    ]),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-aceS1zkfoa"
      },
      "outputs": [],
      "source": [
        "class STRDataset(Dataset):\n",
        "    def __init__(self, X,y,char_to_idx, max_label_len, label_encoder = None, transform = None):\n",
        "        self.transform = transform\n",
        "        self.img_paths = X\n",
        "        self.labels = y\n",
        "        self.char_to_idx = char_to_idx\n",
        "        self.max_label_len = max_label_len\n",
        "        self.label_encoder = label_encoder\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.label_encoder:\n",
        "            encoded_label, label_len = self.label_encoder(\n",
        "                label,\n",
        "                self.char_to_idx,\n",
        "                self.max_label_len\n",
        "            )\n",
        "        return img, encoded_label, label_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyjWfg5GoPtf"
      },
      "outputs": [],
      "source": [
        "seed = 0\n",
        "val_size = 0.3\n",
        "test_size = 1/3\n",
        "is_shuffle = True\n",
        "X_train,X_val, y_train, y_val = train_test_split(\n",
        "    img_paths,\n",
        "    labels,\n",
        "    test_size=val_size,\n",
        "    random_state=seed,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_val,\n",
        "    y_val,\n",
        "    test_size=test_size,\n",
        "    random_state=seed,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gUmIjq4Aoi6v"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PfowkC2WolG2"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFnD3ZWLlil3"
      },
      "outputs": [],
      "source": [
        "train_dataset = STRDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    encode_label,\n",
        "    data_transform['train']\n",
        ")\n",
        "val_dataset = STRDataset(\n",
        "    X_val,\n",
        "    y_val,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    encode_label,\n",
        "    data_transform['val']\n",
        ")\n",
        "test_dataset = STRDataset(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    char_to_idx,\n",
        "    max_label_len,\n",
        "    encode_label,\n",
        "    data_transform['val']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLjIwnjMpC8R"
      },
      "outputs": [],
      "source": [
        "print(f\"Train dataset: {len(train_dataset)}\")\n",
        "print(f\"Val dataset: {len(val_dataset)}\")\n",
        "print(f\"Test dataset: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXmVO1m5pXIJ"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 32\n",
        "test_batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = train_batch_size,\n",
        "    shuffle = True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = test_batch_size,\n",
        "    shuffle = False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = test_batch_size,\n",
        "    shuffle = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5juHiI8plfO"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels, train_lengths = next(iter(train_loader))\n",
        "\n",
        "def show_batch(imgs,labels):\n",
        "    print(labels)\n",
        "    labels = decode(labels, idx_to_char)\n",
        "    print(labels)\n",
        "    grid = torchvision.utils.make_grid(imgs, nrow = 4, normalize = True)\n",
        "    plt.figure(figsize = (10,20))\n",
        "    plt.imshow(np.transpose(grid, (1,2,0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_batch(train_features, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPvJo_BVsNWk"
      },
      "outputs": [],
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers, dropout = 0.2, unfreeze_layers = 3):\n",
        "        super(CRNN, self).__init__()\n",
        "        backbone = timm.create_model(\n",
        "            'resnet101',\n",
        "            pretrained = True,\n",
        "            in_chans = 1\n",
        "        )\n",
        "\n",
        "        modules = list(backbone.children())[:-2]\n",
        "        modules.append(nn.AdaptiveAvgPool2d((1,None)))\n",
        "        self.backbone = nn.Sequential(*modules)\n",
        "\n",
        "        for parameter in self.backbone[-unfreeze_layers:].parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "        self.mapSeq = nn.Sequential(\n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            512,\n",
        "            hidden_size,\n",
        "            n_layers,\n",
        "            batch_first = True,\n",
        "            dropout = dropout if n_layers > 1 else 0,\n",
        "            bidirectional = True\n",
        "        )\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, vocab_size),\n",
        "            nn.LogSoftmax(dim = 2)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        # print(x.shape)\n",
        "        x = self.backbone(x)\n",
        "        # print(x.shape)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), x.size(1), -1)\n",
        "        # print(x.shape)\n",
        "        x = self.mapSeq(x)\n",
        "        # print(x.shape)\n",
        "        x, _ = self.lstm(x)\n",
        "        # print(x.shape)\n",
        "        x = self.layer_norm(x)\n",
        "        # print(x.shape)\n",
        "        x = self.out(x)\n",
        "        # print(x.shape)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIaXHJWOhDlU"
      },
      "outputs": [],
      "source": [
        "hidden_size = 256\n",
        "n_layers = 3\n",
        "dropout = 0.2\n",
        "unfreeze_layers = 3\n",
        "model = CRNN(vocab_size, hidden_size, n_layers, dropout, unfreeze_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGPHPVKVhJDS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dummy_tensor = torch.rand((32,1,100,420)).to(device)\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_tensor)\n",
        "\n",
        "print('Output shape', output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHLOneW0sJh9"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Anncs0dgsLKJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, labels_len in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels_len = labels_len.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            # Calculate input lengths for each sample in the batch\n",
        "            # Assuming 'inputs' has shape (sequence_length, batch_size, input_features)\n",
        "            logits_lens = torch.full(\n",
        "                size=(outputs.size(1),),  # batch_size\n",
        "                fill_value=outputs.size(0),  # sequence_length\n",
        "                dtype=torch.long\n",
        "            ).to(device)\n",
        "\n",
        "\n",
        "\n",
        "            loss = criterion(outputs, labels, logits_lens, labels_len)\n",
        "            losses.append(loss.item())\n",
        "        loss = np.mean(losses)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18c3FPpRYmUp"
      },
      "outputs": [],
      "source": [
        "def fit(model, dataloader, criterion, optimizer, device, scheduler, epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "        model.train()\n",
        "        for idx, (inputs, labels, labels_len) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            labels_len = labels_len.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            logits_lens = torch.full(\n",
        "                size = (outputs.size(1),),\n",
        "                fill_value = outputs.size(0),\n",
        "                dtype = torch.long\n",
        "            ).to(device)\n",
        "\n",
        "            loss = criterion(outputs, labels, logits_lens, labels_len)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss = evaluate(\n",
        "            model,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            device\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"epoch {epoch + 1}:\\tTrain loss: {train_loss:.4f}\\t Val loss: {val_loss:.4f}\")\n",
        "        scheduler.step(val_loss)\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EfiE0cyaG8I"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "lr = 0.001\n",
        "weight_decay = 1e-5\n",
        "scheduler_step_size =  epochs * 0.4\n",
        "criterion = nn.CTCLoss(\n",
        "    blank = char_to_idx[blank_char],\n",
        "    zero_infinity = True\n",
        ")\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr = lr,\n",
        "    weight_decay = weight_decay\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size = scheduler_step_size,\n",
        "    gamma = 0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDMKa8Dsfvd9"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uOLPHj8hNHF"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    device,\n",
        "    scheduler,\n",
        "    epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBwdgZZUkwN6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('train loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.plot(train_losses, label = 'train')\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('val loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.plot(val_losses, label = 'val')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrhNAGngYYsM"
      },
      "outputs": [],
      "source": [
        "train_loss = evaluate(\n",
        "    model,\n",
        "    train_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "val_loss = evaluate(\n",
        "    model,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")\n",
        "\n",
        "test_loss = evaluate(\n",
        "    model,\n",
        "    test_loader,\n",
        "    criterion,\n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VDTTKNosm9n"
      },
      "outputs": [],
      "source": [
        "print(train_loss)\n",
        "print(val_loss)\n",
        "print(test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoMSfqyat_FH"
      },
      "outputs": [],
      "source": [
        "save_path = \"models/ocr_crnn_base_best.pt\"\n",
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    save_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0V57GgsoK42"
      },
      "source": [
        "# End-to-End Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4MSCLy_uJF_"
      },
      "outputs": [],
      "source": [
        "model = CRNN(vocab_size, hidden_size, n_layers, dropout, unfreeze_layers)\n",
        "model.load_state_dict(torch.load(save_path))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwkJvu7mx7Df"
      },
      "outputs": [],
      "source": [
        "chars = '0123456789abcdefghijklmnopqrstuvwxyz-'\n",
        "vocab_size = len(chars)\n",
        "char_to_idx = {char: idx + 1 for idx,char in enumerate(sorted(chars))}\n",
        "idx_to_char = {idx: char for char,idx in char_to_idx.items()}\n",
        "\n",
        "hidden_size = 256\n",
        "n_layers = 3\n",
        "unfreeze_layers = 3\n",
        "dropout_prob = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "crnn_model = CRNN(\n",
        "    vocab_size,\n",
        "    hidden_size,\n",
        "    n_layers,\n",
        "    dropout_prob,\n",
        "    unfreeze_layers\n",
        ").to(device)\n",
        "crnn_model.load_state_dict(torch.load(save_path))\n",
        "crnn_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TcbwGNQ0vPp"
      },
      "outputs": [],
      "source": [
        "def text_detection(img_path, text_det_model):\n",
        "    text_det_results = text_det_model(img_path, verbose = False)[0]\n",
        "\n",
        "    bboxes = text_det_results.boxes.xyxy.tolist()\n",
        "    classes = text_det_results.boxes.cls.tolist()\n",
        "    names = text_det_results.names\n",
        "    scores = text_det_results.boxes.conf.tolist()\n",
        "\n",
        "    return bboxes, classes,names, scores\n",
        "\n",
        "def text_recognition(img, data_transforms, text_reg_model, idx_to_char, device):\n",
        "    transformed_image = data_transforms(img)\n",
        "    transformed_image = transformed_image.unsqueeze(0).to(device)\n",
        "    text_reg_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = text_reg_model(transformed_image).detach().cpu()\n",
        "    text = decode(logits.permute(1,0,2).argmax(2), idx_to_char)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU2YB0rP8S9J"
      },
      "outputs": [],
      "source": [
        "def visualize_detections(img, detections):\n",
        "    plt.figure(figsize = (12,8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    for bbox, detected_class, confidence, transcribed_text in detections:\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        plt.gca().add_patch(\n",
        "            plt.Rectangle(\n",
        "                (x1, y1),\n",
        "                x2 - x1,\n",
        "                y2 - y1,\n",
        "                fill=False,\n",
        "                edgecolor='red',\n",
        "                linewidth=2\n",
        "            )\n",
        "        )\n",
        "\n",
        "        plt.text(\n",
        "            x1, y1 - 10,\n",
        "            f\"{detected_class}: {confidence:.2f}\\n{transcribed_text}\",\n",
        "            fontsize = 9,\n",
        "            bbox = dict(facecolor='red', alpha = 0.5)\n",
        "        )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3i2oFDYp-TA"
      },
      "outputs": [],
      "source": [
        "def predict(img_path, data_transform, text_det_model, text_reg_model, idx_to_char, device):\n",
        "    bboxes, classes, names, confs = text_detection(img_path, text_det_model)\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    for bbox, cls, conf in zip(bboxes, classes, confs):\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        confidence = conf\n",
        "        detected_class = cls\n",
        "        name = names[int(cls)]\n",
        "\n",
        "        cropped_image = img.crop((x1,y1,x2,y2))\n",
        "\n",
        "        transcribed_text = text_recognition(\n",
        "            cropped_image,\n",
        "            data_transform,\n",
        "            text_reg_model,\n",
        "            idx_to_char,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        predictions.append((bbox, name, confidence, transcribed_text))\n",
        "    visualize_detections(img, predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txmY4vWbrVgx"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model_path = 'models/yolov8/detect/train/weights/best.pt'\n",
        "yolo_model = YOLO(model_path)\n",
        "\n",
        "metrics = yolo_model.val(\n",
        "    project = 'models',\n",
        "    name = 'yolov8/detect/val',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2EQNms9qrHi"
      },
      "outputs": [],
      "source": [
        "img_dir = 'data/icdar2003/SceneTrialTrain/lfsosa_12.08.2002'\n",
        "inf_transforms = data_transform['val']\n",
        "\n",
        "for img_path in os.listdir(img_dir):\n",
        "    img_path = os.path.join(img_dir, img_path)\n",
        "\n",
        "    predictions = predict(\n",
        "        img_path,\n",
        "        data_transform = inf_transforms,\n",
        "        text_det_model = yolo_model,\n",
        "        text_reg_model = crnn_model,\n",
        "        idx_to_char = idx_to_char,\n",
        "        device = device\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGuPxHCCujFI"
      },
      "outputs": [],
      "source": [
        "!pip install pyspellchecker"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
